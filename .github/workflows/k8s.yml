name: k8s

on:
  push:
    branches: [main, develop]
  pull_request:
  workflow_dispatch:
    inputs:
      server_dry_run:
        description: "Run kubectl apply --dry-run=server on a kind cluster"
        required: false
        default: "false"

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: "1.22.x"

      - name: Install kubeconform
        run: |
          go install github.com/yannh/kubeconform/cmd/kubeconform@latest
          echo "$HOME/go/bin" >> "$GITHUB_PATH"

      - name: Render manifests
        run: |
          kubectl kustomize infra/k8s/base > /tmp/k8s-base.yaml
          kubectl kustomize infra/k8s/overlays/minikube > /tmp/k8s-minikube.yaml
          kubectl kustomize infra/k8s/overlays/minikube-with-zitadel > /tmp/k8s-minikube-with-zitadel.yaml

      - name: Kubeconform (base)
        run: kubeconform -strict -ignore-missing-schemas /tmp/k8s-base.yaml

      - name: Kubeconform (minikube)
        run: kubeconform -strict -ignore-missing-schemas /tmp/k8s-minikube.yaml

      - name: Kubeconform (minikube-with-zitadel)
        run: kubeconform -strict -ignore-missing-schemas /tmp/k8s-minikube-with-zitadel.yaml

      - name: Trivy config scan (base) [HIGH - warning]
        uses: aquasecurity/trivy-action@0.28.0
        with:
          scan-type: config
          scan-ref: /tmp/k8s-base.yaml
          format: table
          severity: HIGH
          exit-code: "0"

      - name: Trivy config scan (minikube) [HIGH - warning]
        uses: aquasecurity/trivy-action@0.28.0
        with:
          scan-type: config
          scan-ref: /tmp/k8s-minikube.yaml
          format: table
          severity: HIGH
          exit-code: "0"

      - name: Trivy config scan (minikube-with-zitadel) [HIGH - warning]
        uses: aquasecurity/trivy-action@0.28.0
        with:
          scan-type: config
          scan-ref: /tmp/k8s-minikube-with-zitadel.yaml
          format: table
          severity: HIGH
          exit-code: "0"

      - name: Trivy config scan (base) [CRITICAL - block]
        id: trivy_critical_base
        continue-on-error: true
        uses: aquasecurity/trivy-action@0.28.0
        with:
          scan-type: config
          scan-ref: /tmp/k8s-base.yaml
          format: table
          severity: CRITICAL
          exit-code: "1"

      - name: Trivy config scan (minikube) [CRITICAL - block]
        id: trivy_critical_minikube
        continue-on-error: true
        uses: aquasecurity/trivy-action@0.28.0
        with:
          scan-type: config
          scan-ref: /tmp/k8s-minikube.yaml
          format: table
          severity: CRITICAL
          exit-code: "1"

      - name: Trivy config scan (minikube-with-zitadel) [CRITICAL - block]
        id: trivy_critical_minikube_with_zitadel
        continue-on-error: true
        uses: aquasecurity/trivy-action@0.28.0
        with:
          scan-type: config
          scan-ref: /tmp/k8s-minikube-with-zitadel.yaml
          format: table
          severity: CRITICAL
          exit-code: "1"

      - name: Security gate (Trivy CRITICAL)
        if: always()
        run: |
          failed=0

          if [[ "${{ steps.trivy_critical_base.outcome }}" == "failure" ]]; then
            echo "::error::CRITICAL findings detected in /tmp/k8s-base.yaml"
            failed=1
          fi

          if [[ "${{ steps.trivy_critical_minikube.outcome }}" == "failure" ]]; then
            echo "::error::CRITICAL findings detected in /tmp/k8s-minikube.yaml"
            failed=1
          fi

          if [[ "${{ steps.trivy_critical_minikube_with_zitadel.outcome }}" == "failure" ]]; then
            echo "::error::CRITICAL findings detected in /tmp/k8s-minikube-with-zitadel.yaml"
            failed=1
          fi

          if [[ "$failed" == "1" ]]; then
            echo "Security gate failed: CRITICAL findings detected."
            exit 1
          fi

          echo "Security gate passed: no CRITICAL findings detected."

  runtime-smoke:
    name: Runtime smoke test (kind)
    needs: validate
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: read
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4

      - name: Create kind cluster
        uses: helm/kind-action@v1
        with:
          cluster_name: collector-smoke
          wait: 120s

      - name: Configure GHCR pull secret (for kind)
        env:
          GHCR_USERNAME: ${{ github.actor }}
          GHCR_PASSWORD: ${{ secrets.GHCR_PULL_TOKEN != '' && secrets.GHCR_PULL_TOKEN || github.token }}
        run: |
          set -euo pipefail

          kubectl create namespace collector --dry-run=client -o yaml | kubectl apply -f -

          kubectl -n collector create secret docker-registry ghcr-creds \
            --docker-server=ghcr.io \
            --docker-username="$GHCR_USERNAME" \
            --docker-password="$GHCR_PASSWORD" \
            --dry-run=client -o yaml | kubectl apply -f -

          for i in {1..20}; do
            if kubectl -n collector get serviceaccount default >/dev/null 2>&1; then
              break
            fi
            sleep 1
          done

          kubectl -n collector patch serviceaccount default -p '{"imagePullSecrets":[{"name":"ghcr-creds"}]}'

      - name: Deploy manifests (GHCR latest)
        run: kubectl apply -k infra/k8s/overlays/ci-smoke-ghcr

      - name: Wait for Postgres
        run: kubectl -n collector rollout status statefulset/postgres --timeout=180s

      - name: Wait for migrations
        run: |
          set -euo pipefail

          timeout_seconds=300
          end=$((SECONDS + timeout_seconds))

          while (( SECONDS < end )); do
            if kubectl -n collector get job collector-api-migrate -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null | grep -q "True"; then
              echo "Migrations job completed"
              exit 0
            fi

            if kubectl -n collector get job collector-api-migrate -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null | grep -q "True"; then
              echo "::error::Migrations job failed"
              kubectl -n collector logs -l app=collector-api-migrate --all-containers --tail=200 || true
              kubectl -n collector describe job collector-api-migrate || true
              kubectl -n collector describe pods -l app=collector-api-migrate || true
              exit 1
            fi

            if kubectl -n collector get pods -l app=collector-api-migrate -o jsonpath='{range .items[*].status.containerStatuses[*]}{.state.waiting.reason}{"\n"}{end}' 2>/dev/null | grep -qE '^(ErrImagePull|ImagePullBackOff)$'; then
              echo "::error::Migrations pod is stuck pulling the image (ErrImagePull/ImagePullBackOff)."
              kubectl -n collector describe pods -l app=collector-api-migrate || true
              exit 1
            fi

            sleep 5
          done

          echo "::error::Timed out waiting for migrations job to complete."
          kubectl -n collector describe job collector-api-migrate || true
          kubectl -n collector describe pods -l app=collector-api-migrate || true
          exit 1

      - name: Wait for API & Frontend
        run: |
          kubectl -n collector rollout status deployment/collector-api --timeout=180s
          kubectl -n collector rollout status deployment/collector-front --timeout=180s

      - name: Smoke test endpoints
        run: |
          set -euo pipefail

          kubectl -n collector port-forward svc/collector-api 3000:3000 >/tmp/pf-api.log 2>&1 &
          pf_api=$!
          kubectl -n collector port-forward svc/collector-front 3001:3000 >/tmp/pf-front.log 2>&1 &
          pf_front=$!

          cleanup() {
            kill "$pf_api" "$pf_front" 2>/dev/null || true
          }
          trap cleanup EXIT

          for i in {1..30}; do
            if curl -fsS http://localhost:3000/ready >/dev/null; then
              echo "API is ready"
              break
            fi
            sleep 2
          done
          curl -fsS http://localhost:3000/ready >/dev/null

          for i in {1..30}; do
            if curl -fsS http://localhost:3001/health >/dev/null; then
              echo "Frontend is healthy"
              break
            fi
            sleep 2
          done
          curl -fsS http://localhost:3001/health >/dev/null

      - name: Dump logs on failure
        if: failure()
        run: |
          kubectl -n collector get pods -o wide || true
          kubectl -n collector logs deployment/collector-api -c api --tail=200 || true
          kubectl -n collector logs deployment/collector-front -c frontend --tail=200 || true
          kubectl -n collector logs -l app=collector-api-migrate --tail=200 || true
          kubectl -n collector describe pods || true

  server-dry-run:
    if: github.event_name == 'workflow_dispatch' && github.event.inputs.server_dry_run == 'true'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup kubectl
        uses: azure/setup-kubectl@v4

      - name: Create kind cluster
        uses: helm/kind-action@v1

      - name: Render manifests
        run: kubectl kustomize infra/k8s/overlays/minikube > /tmp/k8s.yaml

      - name: Server-side dry-run apply
        run: kubectl apply --dry-run=server -f /tmp/k8s.yaml
